{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPURTeN6ckJFHLfJGZ0vkhr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blacktalenthubs/data-engineering-track/blob/main/week4_spark_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPn0ZoaBQkhI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Week 4: PySpark and DataFrame Operations\n",
        "\n",
        "#### Topics Covered:\n",
        "\n",
        "1. **Introduction to PySpark:**\n",
        "   - **Definition:** PySpark is the Python API for Apache Spark, an open-source, distributed computing system. It enables data engineers to process large datasets across a distributed cluster of computers.\n",
        "   - **Example:**\n",
        "     ```python\n",
        "     from pyspark.sql import SparkSession\n",
        "\n",
        "     # Initialize Spark session\n",
        "     spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
        "     ```\n",
        "\n",
        "2. **Setting up PySpark Environment:**\n",
        "   - **Instructions:**\n",
        "     - Install PySpark: `pip install pyspark`\n",
        "     - Set up Spark session.\n",
        "     - Example:\n",
        "       ```python\n",
        "       from pyspark.sql import SparkSession\n",
        "\n",
        "       spark = SparkSession.builder \\\n",
        "           .appName(\"PySparkSetup\") \\\n",
        "           .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "           .getOrCreate()\n",
        "       ```\n",
        "\n",
        "3. **Working with Spark DataFrames:**\n",
        "   - **Creating DataFrames:**\n",
        "     - Definition: DataFrames are distributed collections of data organized into named columns.\n",
        "     - Example:\n",
        "       ```python\n",
        "       from pyspark.sql import Row\n",
        "\n",
        "       # Create DataFrame from a list of Rows\n",
        "       data = [Row(name=\"Alice\", age=29), Row(name=\"Bob\", age=31)]\n",
        "       df = spark.createDataFrame(data)\n",
        "       df.show()\n",
        "       ```\n",
        "\n",
        "4. **DataFrame Operations:**\n",
        "   - **Filtering:**\n",
        "     - Example:\n",
        "       ```python\n",
        "       # Filter rows where age is greater than 30\n",
        "       df.filter(df.age > 30).show()\n",
        "       ```\n",
        "\n",
        "   - **Aggregations:**\n",
        "     - Example:\n",
        "       ```python\n",
        "       # Calculate average age\n",
        "       df.groupBy().avg(\"age\").show()\n",
        "       ```\n",
        "\n",
        "   - **Joins:**\n",
        "     - Example:\n",
        "       ```python\n",
        "       # Example join operation\n",
        "       df1 = spark.createDataFrame([Row(id=1, value=\"A\"), Row(id=2, value=\"B\")])\n",
        "       df2 = spark.createDataFrame([Row(id=1, name=\"Alice\"), Row(id=2, name=\"Bob\")])\n",
        "       df1.join(df2, df1.id == df2.id).show()\n",
        "       ```\n",
        "\n",
        "5. **Handling Large Datasets with PySpark:**\n",
        "   - **Definition:** Techniques to efficiently handle large datasets using PySpark, leveraging Spark's distributed computing capabilities.\n",
        "   - **Example:**\n",
        "     ```python\n",
        "     # Read large dataset from a file\n",
        "     large_df = spark.read.csv(\"large_dataset.csv\", header=True, inferSchema=True)\n",
        "     ```\n",
        "\n",
        "6. **Performance Optimization in PySpark:**\n",
        "   - **Definition:** Strategies to optimize PySpark applications for better performance.\n",
        "   - **Techniques:**\n",
        "     - Use `persist` and `cache` to store intermediate results.\n",
        "     - Use appropriate data partitioning.\n",
        "     - Example:\n",
        "       ```python\n",
        "       # Cache DataFrame\n",
        "       df.cache()\n",
        "       ```\n",
        "\n",
        "#### Mini Project:\n",
        "\n",
        "**Description:**\n",
        "- Use PySpark to load a large dataset from the API endpoints created in Week 3, perform various DataFrame operations such as filtering, aggregations, and joins, and write the processed data back to a file or database.\n",
        "\n",
        "**Steps:**\n",
        "1. **Fetch Data from API Endpoints:**\n",
        "   - Use the `requests` library to fetch data from the API endpoints.\n",
        "   - Example:\n",
        "     ```python\n",
        "     import requests\n",
        "     import json\n",
        "\n",
        "     # Fetch user data from API\n",
        "     response = requests.get('http://127.0.0.1:5000/users')\n",
        "     users_data = response.json()\n",
        "     ```\n",
        "\n",
        "2. **Load Data into PySpark DataFrames:**\n",
        "   - Convert the fetched JSON data to a PySpark DataFrame.\n",
        "   - Example:\n",
        "     ```python\n",
        "     from pyspark.sql import SparkSession\n",
        "\n",
        "     # Initialize Spark session\n",
        "     spark = SparkSession.builder.appName(\"PySparkProject\").getOrCreate()\n",
        "\n",
        "     # Create DataFrame from JSON data\n",
        "     users_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(users_data)]))\n",
        "     users_df.show()\n",
        "     ```\n",
        "\n",
        "3. **Perform DataFrame Operations:**\n",
        "   - **Filtering:**\n",
        "     ```python\n",
        "     # Filter users by age\n",
        "     users_df.filter(users_df.age > 30).show()\n",
        "     ```\n",
        "\n",
        "   - **Aggregations:**\n",
        "     ```python\n",
        "     # Calculate average user age\n",
        "     users_df.groupBy().avg(\"age\").show()\n",
        "     ```\n",
        "\n",
        "   - **Joins:**\n",
        "     - Fetch and load additional data, then perform join operations.\n",
        "     - Example:\n",
        "       ```python\n",
        "       # Fetch account data from API\n",
        "       response = requests.get('http://127.0.0.1:5000/accounts')\n",
        "       accounts_data = response.json()\n",
        "\n",
        "       # Create DataFrame from JSON data\n",
        "       accounts_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(accounts_data)]))\n",
        "\n",
        "       # Perform join operation\n",
        "       joined_df = users_df.join(accounts_df, users_df.user_id == accounts_df.user_id)\n",
        "       joined_df.show()\n",
        "       ```\n",
        "\n",
        "4. **Write Processed Data to File or Database:**\n",
        "   - Example:\n",
        "     ```python\n",
        "     # Write DataFrame to CSV file\n",
        "     joined_df.write.csv(\"processed_data.csv\", header=True)\n",
        "     ```\n",
        "\n",
        "### Week 4: PySpark and DataFrame Operations\n",
        "\n",
        "#### Topics Covered:\n",
        "\n",
        "1. **Introduction to PySpark**\n",
        "2. **Setting up PySpark Environment**\n",
        "3. **Working with Spark DataFrames**\n",
        "4. **DataFrame Operations (filtering, aggregations, joins)**\n",
        "5. **Handling Large Datasets with PySpark**\n",
        "6. **Performance Optimization in PySpark**\n",
        "\n",
        "#### Mini Project:\n",
        "\n",
        "**Description:**\n",
        "- Use PySpark to load a large dataset from the API endpoints created in Week 3, perform various DataFrame operations such as filtering, aggregations, and joins, and write the processed data back to a file or database.\n",
        "\n",
        "**Outcome:**\n",
        "- Students will learn how to use PySpark for large-scale data processing and become familiar with DataFrame operations, preparing them for real-world data engineering tasks.\n",
        "\n",
        "### Complete Code Example\n",
        "\n",
        "Here's the complete code example for the mini-project, reading data from the endpoints, and performing the required DataFrame operations:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"PySparkProject\").getOrCreate()\n",
        "\n",
        "# Function to fetch data from API and return as JSON\n",
        "def fetch_data(url):\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "# Fetch data from the API endpoints\n",
        "users_data = fetch_data('http://127.0.0.1:5000/users')\n",
        "accounts_data = fetch_data('http://127.0.0.1:5000/accounts')\n",
        "transactions_data = fetch_data('http://127.0.0.1:5000/transactions')\n",
        "cards_data = fetch_data('http://127.0.0.1:5000/cards')\n",
        "payments_data = fetch_data('http://127.0.0.1:5000/payments')\n",
        "\n",
        "# Convert JSON data to PySpark DataFrames\n",
        "users_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(users_data)]))\n",
        "accounts_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(accounts_data)]))\n",
        "transactions_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(transactions_data)]))\n",
        "cards_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(cards_data)]))\n",
        "payments_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(payments_data)]))\n",
        "\n",
        "# Perform DataFrame operations\n",
        "# Filtering: Users with age greater than 30 (assuming 'age' field exists)\n",
        "# users_df_filtered = users_df.filter(users_df.age > 30)\n",
        "\n",
        "# Aggregations: Calculate average balance in accounts\n",
        "average_balance_df = accounts_df.groupBy().avg(\"balance\")\n",
        "average_balance_df.show()\n",
        "\n",
        "# Join operations\n",
        "# Join users with accounts on user_id\n",
        "user_account_df = users_df.join(accounts_df, users_df.user_id == accounts_df.user_id, 'inner')\n",
        "\n",
        "# Join transactions with accounts on account_id\n",
        "account_transaction_df = accounts_df.join(transactions_df, accounts_df.account_id == transactions_df.account_id, 'inner')\n",
        "\n",
        "# Join cards with accounts on account_id\n",
        "account_card_df = accounts_df.join(cards_df, accounts_df.account_id == cards_df.account_id, 'inner')\n",
        "\n",
        "# Join payments with users on user_id and cards on card_id\n",
        "user_card_payment_df = payments_df.join(users_df, payments_df.user_id == users_df.user_id, 'inner') \\\n",
        "                                  .join(cards_df, payments_df.card_id == cards_df.card_id, 'inner')\n",
        "\n",
        "# Final join to combine all data into a single DataFrame\n",
        "final_df = user_account_df.join(account_transaction_df, 'account_id', 'inner') \\\n",
        "                          .join(account_card_df, 'account_id', 'inner') \\\n",
        "                          .join(user_card_payment_df, ['user_id', 'card_id'], 'inner')\n",
        "\n",
        "final_df.show()\n",
        "\n",
        "# Write the processed data to a CSV file\n",
        "final_df.write.csv(\"processed_data.csv\", header=True)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Fetch Data from API Endpoints:**\n",
        "   - The `fetch_data` function fetches data from a given API endpoint and returns it as JSON.\n",
        "\n",
        "2. **Convert JSON Data to PySpark DataFrames:**\n",
        "   - The fetched JSON data is converted into PySpark DataFrames.\n",
        "\n",
        "3. **Perform DataFrame Operations:**\n",
        "   - Filtering is demonstrated (commented out as the 'age' field may not exist in the provided data).\n",
        "   - Aggregations calculate the average balance in accounts.\n",
        "   - Multiple joins are performed to combine users, accounts, transactions, cards, and payments data into a single DataFrame.\n",
        "\n",
        "4. **Write Processed Data to File:**\n",
        "   - The final DataFrame is written to a CSV file.\n",
        "\n",
        "5. **Stop Spark Session:**\n",
        "   - The Spark session is stopped.\n",
        "\n",
        "By keeping all the code in one file, you can read and understand the flow of data from fetching to processing and finally writing the output. This ensures the continuity and coherence of the project.\n",
        "\n",
        "\n",
        "### Week 4: PySpark and DataFrame Operations\n",
        "\n",
        "#### Topics Covered:\n",
        "\n",
        "1. **Introduction to PySpark**\n",
        "2. **Setting up PySpark Environment**\n",
        "3. **Working with Spark DataFrames**\n",
        "4. **DataFrame Operations (filtering, aggregations, joins)**\n",
        "5. **Handling Large Datasets with PySpark**\n",
        "6. **Performance Optimization in PySpark**\n",
        "\n",
        "#### Mini Project:\n",
        "\n",
        "**Description:**\n",
        "- Use PySpark to load a large dataset from the API endpoints created in Week 3, perform various DataFrame operations such as filtering, aggregations, and joins, and write the processed data back to a file or database.\n",
        "\n",
        "**Outcome:**\n",
        "- Students will learn how to use PySpark for large-scale data processing and become familiar with DataFrame operations, preparing them for real-world data engineering tasks.\n",
        "\n",
        "### Complete Code Example with SQLite and Airflow\n",
        "\n",
        "1. **Data Processing with PySpark:**\n",
        "\n",
        "```python\n",
        "import requests\n",
        "import json\n",
        "import sqlite3\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def fetch_data(url):\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "def process_data():\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.appName(\"PySparkProject\").getOrCreate()\n",
        "\n",
        "    # Fetch data from the API endpoints\n",
        "    users_data = fetch_data('http://127.0.0.1:5000/users')\n",
        "    accounts_data = fetch_data('http://127.0.0.1:5000/accounts')\n",
        "    transactions_data = fetch_data('http://127.0.0.1:5000/transactions')\n",
        "    cards_data = fetch_data('http://127.0.0.1:5000/cards')\n",
        "    payments_data = fetch_data('http://127.0.0.1:5000/payments')\n",
        "\n",
        "    # Convert JSON data to PySpark DataFrames\n",
        "    users_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(users_data)]))\n",
        "    accounts_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(accounts_data)]))\n",
        "    transactions_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(transactions_data)]))\n",
        "    cards_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(cards_data)]))\n",
        "    payments_df = spark.read.json(spark.sparkContext.parallelize([json.dumps(payments_data)]))\n",
        "\n",
        "    # Perform DataFrame operations\n",
        "    # Join users with accounts on user_id\n",
        "    user_account_df = users_df.join(accounts_df, users_df.user_id == accounts_df.user_id, 'inner')\n",
        "\n",
        "    # Join transactions with accounts on account_id\n",
        "    account_transaction_df = accounts_df.join(transactions_df, accounts_df.account_id == transactions_df.account_id, 'inner')\n",
        "\n",
        "    # Join cards with accounts on account_id\n",
        "    account_card_df = accounts_df.join(cards_df, accounts_df.account_id == cards_df.account_id, 'inner')\n",
        "\n",
        "    # Join payments with users on user_id and cards on card_id\n",
        "    user_card_payment_df = payments_df.join(users_df, payments_df.user_id == users_df.user_id, 'inner') \\\n",
        "                                      .join(cards_df, payments_df.card_id == cards_df.card_id, 'inner')\n",
        "\n",
        "    # Final join to combine all data into a single DataFrame\n",
        "    final_df = user_account_df.join(account_transaction_df, 'account_id', 'inner') \\\n",
        "                              .join(account_card_df, 'account_id', 'inner') \\\n",
        "                              .join(user_card_payment_df, ['user_id', 'card_id'], 'inner')\n",
        "\n",
        "    # Write the processed data to a SQLite database\n",
        "    database_path = 'processed_data.db'\n",
        "    final_df.write \\\n",
        "        .format(\"jdbc\") \\\n",
        "        .option(\"url\", f\"jdbc:sqlite:{database_path}\") \\\n",
        "        .option(\"dbtable\", \"final_data\") \\\n",
        "        .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .save()\n",
        "\n",
        "    # Stop Spark session\n",
        "    spark.stop()\n",
        "```\n",
        "\n",
        "2. **Apache Airflow DAG:**\n",
        "\n",
        "```python\n",
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define the default_args dictionary\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "# Define the DAG\n",
        "dag = DAG(\n",
        "    'data_processing_dag',\n",
        "    default_args=default_args,\n",
        "    description='A DAG for processing data with PySpark and saving to SQLite',\n",
        "    schedule_interval=timedelta(days=1),\n",
        ")\n",
        "\n",
        "def fetch_and_process_data():\n",
        "    process_data()\n",
        "\n",
        "# Define the task using PythonOperator\n",
        "fetch_and_process_data_task = PythonOperator(\n",
        "    task_id='fetch_and_process_data',\n",
        "    python_callable=fetch_and_process_data,\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "# Set the task dependencies\n",
        "fetch_and_process_data_task\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **PySpark Data Processing:**\n",
        "   - The `fetch_data` function fetches data from a given API endpoint and returns it as JSON.\n",
        "   - The `process_data` function:\n",
        "     - Initializes a Spark session.\n",
        "     - Fetches data from the API endpoints.\n",
        "     - Converts JSON data into PySpark DataFrames.\n",
        "     - Performs join operations to combine users, accounts, transactions, cards, and payments data.\n",
        "     - Writes the final DataFrame to a SQLite database.\n",
        "     - Stops the Spark session.\n",
        "\n",
        "2. **Apache Airflow DAG:**\n",
        "   - Defines a DAG that runs daily.\n",
        "   - Uses `PythonOperator` to run the `fetch_and_process_data` function, which calls `process_data`.\n",
        "   - The `process_data` function is the same PySpark data processing code as above, modularized for reuse.\n",
        "\n",
        "This setup allows you to run the data processing code on a schedule using Apache Airflow, and save the processed data to a SQLite database."
      ],
      "metadata": {
        "id": "e9kaWr7LQlEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k2PSx3jhQ3by"
      }
    }
  ]
}